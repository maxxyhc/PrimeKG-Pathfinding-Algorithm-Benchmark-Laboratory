{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrimeKG Pathfinding Algorithm Benchmark\n\n**Purpose:** Evaluate graph pathfinding algorithms for drug mechanism-of-action (MoA) discovery by comparing predicted mechanistic pathways against curated ground truth pathways from DrugMechDB.\n\n**Algorithms Benchmarked:**\n1. **Shortest Path** \u2014 Unweighted BFS baseline\n2. **Hub-Penalized Shortest Path** \u2014 Penalizes high-degree hub nodes\n3. **PageRank-Inverse Shortest Path** \u2014 Prefers low-centrality (specific) nodes\n4. **Learned Embeddings + A\\*** \u2014 Supervised edge weights from spectral embeddings\n5. **Semantic Bridging** \u2014 TF-IDF cosine similarity edge weighting\n\n**Evaluation Metrics (9 total):**\n\n| Category | Metrics | What It Measures |\n|----------|---------|------------------|\n| Node Accuracy | Precision, Recall, F1 | Are the right nodes in the path? |\n| Target Finding | Hits@1, Hits@3, Hits@5 | Does the path reach the disease? |\n| Mechanistic Quality | Relation Accuracy, Edit Distance, Hub Ratio | Is the path biologically valid? |\n\n> **Note:** This notebook is fully self-contained \u2014 all algorithms, evaluation helpers, and metrics are defined inline. No external `.py` imports required.\n\n> **Filter:** Only ground truth pathways with **\u2265 4 nodes** are included (short 2-3 node drug\u2192protein\u2192disease paths are excluded).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport heapq\nimport time\nimport warnings\n\nfrom typing import Dict, List, Tuple\nfrom collections import Counter\nfrom scipy.sparse.linalg import eigsh\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nwarnings.filterwarnings('ignore')\nprint(\"\u2713 All imports loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURE PATHS ===\n# Update DATA_DIR to your project root\nDATA_DIR = '.'\n\nPATHS = {\n    'nodes':              f'{DATA_DIR}/data/raw/nodes.csv',\n    'edges':              f'{DATA_DIR}/data/raw/edges.csv',\n    'ground_truth_nodes': f'{DATA_DIR}/Cleaned_Ground_Truths/ground_truth_final.csv',\n    'ground_truth_edges': f'{DATA_DIR}/Cleaned_Ground_Truths/pathway_edges_final.csv',\n}\n\n# Minimum pathway length filter \u2014 only keep pathways with >= this many nodes\nMIN_PATHWAY_NODES = 4\n\nprint(\"Configuration:\")\nfor name, path in PATHS.items():\n    print(f\"  {name}: {path}\")\nprint(f\"  min_pathway_nodes: {MIN_PATHWAY_NODES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Helpers\n\nAll helper functions for computing metrics \u2014 inlined so the notebook is fully self-contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# EVALUATION HELPERS\n# ============================================================\n\ndef is_valid_prediction(predicted_ids):\n    \"\"\"Check if prediction is valid (not empty or 'NONE').\"\"\"\n    return predicted_ids and predicted_ids != ['NONE']\n\n\ndef calculate_edit_distance(predicted_ids, ground_truth_ids):\n    \"\"\"\n    Normalized Levenshtein edit distance between two sequences.\n    Returns value in [0, 1] where 0 = identical, 1 = completely different.\n    \"\"\"\n    if not predicted_ids or predicted_ids == ['NONE']:\n        return 1.0\n    m, n = len(predicted_ids), len(ground_truth_ids)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if predicted_ids[i - 1] == ground_truth_ids[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n    return dp[m][n] / max(m, n)\n\n\ndef compute_degree_counts(edges_df):\n    \"\"\"\n    Compute degree count for all nodes from edges DataFrame.\n    Columns: x_index, y_index\n    \"\"\"\n    degree_count = Counter()\n    for _, row in edges_df.iterrows():\n        degree_count[row['x_index']] += 1\n        degree_count[row['y_index']] += 1\n    return degree_count\n\n\ndef compute_hub_threshold(degree_count, percentile=95):\n    \"\"\"Compute hub threshold at given percentile of degree distribution.\"\"\"\n    all_degrees = list(degree_count.values())\n    return np.percentile(all_degrees, percentile)\n\n\ndef calculate_hits_at_k(predicted_ids, ground_truth_target, k_values=[1, 3, 5]):\n    \"\"\"\n    Check if the target appears in the last k nodes of the predicted path.\n    \"\"\"\n    hits = {f'hits_at_{k}': 0 for k in k_values}\n    if not is_valid_prediction(predicted_ids):\n        return hits\n    for k in k_values:\n        last_k = predicted_ids[-k:] if len(predicted_ids) >= k else predicted_ids\n        hits[f'hits_at_{k}'] = 1 if ground_truth_target in last_k else 0\n    return hits\n\n\ndef calculate_relation_accuracy(predicted_relations, ground_truth_edge_types):\n    \"\"\"Fraction of predicted edge types that appear in the ground truth set.\"\"\"\n    if not predicted_relations:\n        return 0.0\n    gt_types = set(ground_truth_edge_types)\n    matches = sum(1 for r in predicted_relations if r in gt_types)\n    return matches / len(predicted_relations)\n\n\ndef calculate_path_length_mae(predicted_length, ground_truth_length):\n    \"\"\"Absolute error between predicted and ground truth path lengths.\"\"\"\n    return abs(predicted_length - ground_truth_length)\n\n\ndef calculate_hub_node_ratio(predicted_indices, degree_count, hub_threshold):\n    \"\"\"Fraction of predicted path nodes that are high-degree hubs.\"\"\"\n    if not predicted_indices:\n        return 0.0\n    hub_count = sum(1 for idx in predicted_indices if degree_count.get(idx, 0) >= hub_threshold)\n    return hub_count / len(predicted_indices)\n\n\nprint(\"\u2713 Evaluation helpers loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics\n\nNine metrics across three categories: node accuracy, target finding, and mechanistic quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# EVALUATION METRICS\n# ============================================================\n\ndef metric_precision(predicted_ids, ground_truth_ids):\n    \"\"\"Fraction of predicted nodes that are correct: |pred \u2229 gt| / |pred|\"\"\"\n    if not is_valid_prediction(predicted_ids):\n        return 0.0\n    pred_set, gt_set = set(predicted_ids), set(ground_truth_ids)\n    return len(pred_set & gt_set) / len(pred_set)\n\n\ndef metric_recall(predicted_ids, ground_truth_ids):\n    \"\"\"Fraction of ground truth nodes recovered: |pred \u2229 gt| / |gt|\"\"\"\n    if not is_valid_prediction(predicted_ids):\n        return 0.0\n    pred_set, gt_set = set(predicted_ids), set(ground_truth_ids)\n    return len(pred_set & gt_set) / len(gt_set) if gt_set else 0.0\n\n\ndef metric_f1(predicted_ids, ground_truth_ids):\n    \"\"\"Harmonic mean of precision and recall.\"\"\"\n    p = metric_precision(predicted_ids, ground_truth_ids)\n    r = metric_recall(predicted_ids, ground_truth_ids)\n    if p + r == 0:\n        return 0.0\n    return 2 * p * r / (p + r)\n\n\ndef metric_path_length_accuracy(predicted_length, ground_truth_length):\n    \"\"\"1 - |pred_len - gt_len| / max(pred_len, gt_len). Returns [0,1], 1 = exact.\"\"\"\n    if predicted_length == 0 and ground_truth_length == 0:\n        return 1.0\n    max_len = max(predicted_length, ground_truth_length)\n    if max_len == 0:\n        return 0.0\n    return 1 - abs(predicted_length - ground_truth_length) / max_len\n\n\ndef metric_hub_node_ratio(predicted_indices, degree_count, hub_threshold):\n    \"\"\"Fraction of path nodes that are hubs. Lower is better.\"\"\"\n    if not predicted_indices:\n        return 0.0\n    hub_count = sum(1 for idx in predicted_indices if degree_count.get(idx, 0) >= hub_threshold)\n    return hub_count / len(predicted_indices)\n\n\ndef metric_mrr(predicted_ids, ground_truth_ids):\n    \"\"\"Mean Reciprocal Rank \u2014 1 / rank of first correct node. Higher is better.\"\"\"\n    if not is_valid_prediction(predicted_ids):\n        return 0.0\n    gt_set = set(ground_truth_ids)\n    for rank, node in enumerate(predicted_ids, start=1):\n        if node in gt_set:\n            return 1 / rank\n    return 0.0\n\n\ndef metric_speed(run_fn, *args, **kwargs):\n    \"\"\"Time a function call in milliseconds. Returns (result, elapsed_ms).\"\"\"\n    start = time.perf_counter()\n    result = run_fn(*args, **kwargs)\n    elapsed_ms = (time.perf_counter() - start) * 1000\n    return result, elapsed_ms\n\n\nprint(\"\u2713 Evaluation metrics loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pathfinding Algorithms\n\nAll five algorithms defined inline. They share a common `find_path_engine` (constrained Dijkstra) and `allowed_transition` filter that prevents shortcut edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# SHARED: TRANSITION FILTER & DIJKSTRA ENGINE\n# ============================================================\n\ndef allowed_transition(G, src, u, v) -> bool:\n    \"\"\"\n    Block shortcut transitions:\n      (1) drug \u2192 disease (anywhere in path)\n      (2) drug \u2192 drug (when source is a drug \u2014 prevents drug\u2192drug\u2192disease)\n    \"\"\"\n    u_type = G.nodes[u].get(\"node_type\", \"\")\n    v_type = G.nodes[v].get(\"node_type\", \"\")\n\n    # Ban drug \u2192 disease\n    if u_type == \"drug\" and v_type == \"disease\":\n        return False\n\n    # Ban drug \u2192 drug at first hop from a drug source\n    src_type = G.nodes[src].get(\"node_type\", \"\")\n    if src_type == \"drug\" and u == src and v_type == \"drug\":\n        return False\n\n    return True\n\n\ndef find_path_engine(graph, weighted_graph, source, target, transition_fn):\n    \"\"\"\n    Generic constrained Dijkstra shortest-path engine.\n\n    Parameters\n    ----------\n    graph          : nx.DiGraph \u2014 original graph (for node_type, relation lookup)\n    weighted_graph : nx.DiGraph \u2014 graph with edge 'weight' attribute\n    source, target : int\n    transition_fn  : callable(G, source, u, v) -> bool\n\n    Returns\n    -------\n    path_nodes, relations, total_cost\n    \"\"\"\n    dist = {source: 0.0}\n    parent = {source: None}\n    pq = [(0.0, source)]\n\n    while pq:\n        cur_cost, u = heapq.heappop(pq)\n        if cur_cost != dist.get(u, float('inf')):\n            continue\n        if u == target:\n            break\n        for v in graph.successors(u):\n            if not transition_fn(graph, source, u, v):\n                continue\n            w = weighted_graph[u][v].get(\"weight\", 1.0)\n            new_cost = cur_cost + w\n            if new_cost < dist.get(v, float('inf')):\n                dist[v] = new_cost\n                parent[v] = u\n                heapq.heappush(pq, (new_cost, v))\n\n    if target not in dist:\n        return [], [], float(\"inf\")\n\n    # Reconstruct path\n    path = []\n    cur = target\n    while cur is not None:\n        path.append(cur)\n        cur = parent[cur]\n    path.reverse()\n\n    # Reconstruct relations\n    relations = []\n    for i in range(len(path) - 1):\n        edge_data = graph.get_edge_data(path[i], path[i + 1]) or {}\n        relations.append(edge_data.get(\"relation\", \"unknown\"))\n\n    return path, relations, dist[target]\n\n\nprint(\"\u2713 Shared engine loaded: allowed_transition + find_path_engine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# ALGORITHM 2: Hub-Penalized Weighted Shortest Path\n# ============================================================\n# Weight: w(u,v) = 1 + \u03b1 \u00b7 log(degree(v))\n\nclass HubPenalizedShortestPath:\n    def __init__(self, graph: nx.DiGraph, alpha: float = 0.5):\n        self.graph = graph\n        self.alpha = alpha\n        self.weighted_graph = self._compute_weights()\n\n    def _compute_weights(self) -> nx.DiGraph:\n        G_w = self.graph.copy()\n        degrees = dict(G_w.degree())\n        for u, v in G_w.edges():\n            deg_v = degrees.get(v, 1)\n            G_w[u][v]['weight'] = 1.0 + self.alpha * np.log(max(deg_v, 1))\n            \n        return G_w\n\n    def find_path(self, source: int, target: int):\n        return find_path_engine(self.graph, self.weighted_graph, source, target, allowed_transition)\n\n\nprint(\"\u2713 HubPenalizedShortestPath loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# ALGORITHM 3: PageRank-Inverse Weighted Shortest Path\n# ============================================================\n# Weight: w(u,v) = 1 / (1 + normalized_pagerank(v))\n\nclass PageRankInverseShortestPath:\n    def __init__(self, graph: nx.DiGraph, damping: float = 0.85,\n                 precomputed_pagerank: Dict[int, float] = None):\n        self.graph = graph\n        if precomputed_pagerank is not None:\n            self.pagerank_scores = precomputed_pagerank\n        else:\n            print(\"  Computing PageRank (may take a minute)...\")\n            self.pagerank_scores = nx.pagerank(graph, alpha=damping)\n            print(f\"  PageRank done for {len(self.pagerank_scores):,} nodes\")\n        self.weighted_graph = self._compute_weights()\n\n    def _compute_weights(self) -> nx.DiGraph:\n        G_w = self.graph.copy()\n        max_pr = max(self.pagerank_scores.values())\n        min_pr = min(self.pagerank_scores.values())\n        pr_range = max_pr - min_pr if max_pr > min_pr else 1.0\n        for u, v in G_w.edges():\n            norm_pr = (self.pagerank_scores.get(v, 0) - min_pr) / pr_range\n            G_w[u][v]['weight'] = 1.0 / (1.0 + norm_pr)\n        return G_w\n\n    def find_path(self, source: int, target: int):\n        return find_path_engine(self.graph, self.weighted_graph, source, target, allowed_transition)\n\n\nprint(\"\u2713 PageRankInverseShortestPath loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# ALGORITHM 4: Learned Embeddings + A* with Supervised Edge Weights\n# ============================================================\n\nclass LearnedEmbeddingsAStar:\n    def __init__(self, graph: nx.DiGraph, embedding_dim: int = 64):\n        self.graph = graph\n        self.embedding_dim = embedding_dim\n        self.embeddings = None\n        self.edge_weights = None\n        self.scaler = None\n        self.mlp = None\n        self.degrees = dict(graph.degree())\n\n    def train_embeddings(self) -> Dict[int, np.ndarray]:\n        \"\"\"Compute spectral embeddings (memory-efficient sparse eigensolver).\"\"\"\n        print(\"  Computing spectral embeddings...\")\n        G_undirected = self.graph.to_undirected()\n        largest_cc = max(nx.connected_components(G_undirected), key=len)\n        G_sub = G_undirected.subgraph(largest_cc)\n\n        L = nx.normalized_laplacian_matrix(G_sub)\n        k = min(self.embedding_dim + 1, L.shape[0] - 2)\n        eigenvalues, eigenvectors = eigsh(L, k=k, which='SM')\n\n        node_list = list(G_sub.nodes())\n        self.embeddings = {}\n        for i, node in enumerate(node_list):\n            self.embeddings[node] = eigenvectors[i, 1:]\n\n        for node in self.graph.nodes():\n            if node not in self.embeddings:\n                self.embeddings[node] = np.random.randn(k - 1) * 0.01\n\n        print(f\"  Embeddings: {len(self.embeddings):,} nodes, dim={k-1}\")\n        return self.embeddings\n\n    def _edge_features(self, u: int, v: int) -> np.ndarray:\n        features = []\n        if self.embeddings:\n            emb_u = self.embeddings.get(u, np.zeros(self.embedding_dim))\n            emb_v = self.embeddings.get(v, np.zeros(self.embedding_dim))\n            norm_u, norm_v = np.linalg.norm(emb_u), np.linalg.norm(emb_v)\n            cos_sim = np.dot(emb_u, emb_v) / (norm_u * norm_v) if norm_u > 0 and norm_v > 0 else 0.0\n            features.append(cos_sim)\n            features.append(np.linalg.norm(emb_u - emb_v))\n        features.append(np.log1p(self.degrees.get(u, 0)))\n        features.append(np.log1p(self.degrees.get(v, 0)))\n        features.append(np.log1p(self.degrees.get(u, 1) / max(self.degrees.get(v, 1), 1)))\n        return np.array(features)\n\n    def train_edge_weights(self, training_pathways: List[Dict], negative_ratio: float = 3.0):\n        \"\"\"Train MLP to predict edge weights from features.\"\"\"\n        if self.embeddings is None:\n            self.train_embeddings()\n        print(\"  Training edge weight MLP...\")\n\n        X_train, y_train = [], []\n        positive_edges = set()\n        for pathway in training_pathways:\n            path = pathway['path_nodes']\n            for i in range(len(path) - 1):\n                u, v = path[i], path[i + 1]\n                if self.graph.has_edge(u, v):\n                    positive_edges.add((u, v))\n                    X_train.append(self._edge_features(u, v))\n                    y_train.append(0.1)\n\n        all_edges = list(self.graph.edges())\n        np.random.shuffle(all_edges)\n        n_negative = int(len(positive_edges) * negative_ratio)\n        for u, v in all_edges[:n_negative * 2]:\n            if (u, v) not in positive_edges and len(X_train) < len(positive_edges) + n_negative:\n                X_train.append(self._edge_features(u, v))\n                y_train.append(1.0)\n\n        X_train, y_train = np.array(X_train), np.array(y_train)\n        self.scaler = StandardScaler()\n        X_scaled = self.scaler.fit_transform(X_train)\n\n        self.mlp = MLPRegressor(hidden_layer_sizes=(32, 16), activation='relu',\n                                max_iter=500, early_stopping=True, random_state=42)\n        self.mlp.fit(X_scaled, y_train)\n        print(f\"  MLP trained on {len(X_train)} samples (R\u00b2={self.mlp.score(X_scaled, y_train):.3f})\")\n        self._precompute_edge_weights()\n\n    def _precompute_edge_weights(self):\n        print(\"  Precomputing all edge weights...\")\n        self.edge_weights = {}\n        edges = list(self.graph.edges())\n        X = np.array([self._edge_features(u, v) for u, v in edges])\n        X_scaled = self.scaler.transform(X)\n        weights = np.clip(self.mlp.predict(X_scaled), 0.01, 2.0)\n        for (u, v), w in zip(edges, weights):\n            self.edge_weights[(u, v)] = w\n        print(f\"  Edge weights: {len(self.edge_weights):,} edges\")\n\n    def _heuristic(self, node: int, target: int) -> float:\n        if self.embeddings is None:\n            return 0.0\n        emb_n = self.embeddings.get(node, np.zeros(self.embedding_dim))\n        emb_t = self.embeddings.get(target, np.zeros(self.embedding_dim))\n        return np.linalg.norm(emb_n - emb_t) * 0.1\n\n    def find_path(self, source: int, target: int) -> Tuple[List[int], List[str], float]:\n        \"\"\"A* with learned edge weights and embedding heuristic.\"\"\"\n        if self.edge_weights is None:\n            self.edge_weights = {(u, v): 1.0 for u, v in self.graph.edges()}\n\n        counter = 0\n        open_set = [(self._heuristic(source, target), counter, source, [source], 0.0)]\n        visited = set()\n\n        while open_set:\n            f_score, _, current, path, g_score = heapq.heappop(open_set)\n            if current == target:\n                relations = []\n                for i in range(len(path) - 1):\n                    edge_data = self.graph.get_edge_data(path[i], path[i + 1])\n                    relations.append(edge_data.get('relation', 'unknown'))\n                return path, relations, g_score\n            if current in visited:\n                continue\n            visited.add(current)\n\n            for neighbor in self.graph.neighbors(current):\n                if neighbor in visited:\n                    continue\n                if not allowed_transition(self.graph, source, current, neighbor):\n                    continue\n                edge_weight = self.edge_weights.get((current, neighbor), 1.0)\n                new_g = g_score + edge_weight\n                new_f = new_g + self._heuristic(neighbor, target)\n                counter += 1\n                heapq.heappush(open_set, (new_f, counter, neighbor, path + [neighbor], new_g))\n\n        return [], [], float('inf')\n\n\nprint(\"\u2713 LearnedEmbeddingsAStar loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# ALGORITHM 5: Semantic Bridging with TF-IDF Cosine Similarity\n# ============================================================\n# Weight: w(u,v) = 1 - \u03b2 \u00b7 max(0, cosine_sim(tfidf(u), tfidf(v)))\n\nclass SemanticBridgingPath:\n    def __init__(self, graph: nx.DiGraph, beta: float = 0.3):\n        self.graph = graph\n        self.beta = beta\n        self.embeddings = None\n        self.weighted_graph = None\n        self.descriptions = {n: graph.nodes[n].get('node_name', str(n)) for n in graph.nodes()}\n\n    def compute_embeddings(self) -> Dict[int, np.ndarray]:\n        \"\"\"TF-IDF \u2192 TruncatedSVD embeddings from node names.\"\"\"\n        print(\"  Computing TF-IDF embeddings...\")\n        nodes = list(self.graph.nodes())\n        texts = [self.descriptions[n] for n in nodes]\n\n        vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n        tfidf_matrix = vectorizer.fit_transform(texts)\n\n        n_components = min(64, tfidf_matrix.shape[1] - 1)\n        svd = TruncatedSVD(n_components=n_components, random_state=42)\n        emb_matrix = svd.fit_transform(tfidf_matrix)\n\n        self.embeddings = {node: emb_matrix[i] for i, node in enumerate(nodes)}\n        print(f\"  Embeddings: {len(self.embeddings):,} nodes, dim={n_components}\")\n        return self.embeddings\n\n    def _cosine_similarity(self, emb1, emb2):\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n        if n1 == 0 or n2 == 0:\n            return 0.0\n        return np.dot(emb1, emb2) / (n1 * n2)\n\n    def compute_edge_weights(self) -> nx.DiGraph:\n        \"\"\"Compute semantic similarity-based edge weights.\"\"\"\n        if self.embeddings is None:\n            self.compute_embeddings()\n        print(\"  Computing edge weights...\")\n        self.weighted_graph = self.graph.copy()\n        for u, v in self.weighted_graph.edges():\n            emb_u = self.embeddings.get(u)\n            emb_v = self.embeddings.get(v)\n            if emb_u is not None and emb_v is not None:\n                sim = self._cosine_similarity(emb_u, emb_v)\n                weight = 1.0 - self.beta * max(0, sim)\n            else:\n                weight = 1.0\n            self.weighted_graph[u][v]['weight'] = weight\n        print(f\"  Edge weights: {self.weighted_graph.number_of_edges():,} edges\")\n        return self.weighted_graph\n\n    def find_path(self, source: int, target: int):\n        if self.weighted_graph is None:\n            self.compute_edge_weights()\n        return find_path_engine(self.graph, self.weighted_graph, source, target, allowed_transition)\n\n\nprint(\"\u2713 SemanticBridgingPath loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data\n\nLoad PrimeKG knowledge graph and ground truth pathways. Filter out short pathways (< 4 nodes).\n\n**File formats:**\n- `nodes.csv`: `node_index, node_id, node_type, node_name, node_source`\n- `edges.csv`: `relation, display_relation, x_index, y_index`\n- `ground_truth_final.csv`: `pathway_id, drugmechdb_pathway_id, step_order, node_name, node_id, node_type, node_index, drugmechdb_id`\n- `pathway_edges_final.csv`: `pathway_id, ..., relation, display_relation, ...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PrimeKG graph data\nprint(\"Loading PrimeKG data...\")\nnodes_df = pd.read_csv(PATHS['nodes'])\nedges_df = pd.read_csv(PATHS['edges'])\n\nprint(f\"  Nodes: {len(nodes_df):,}\")\nprint(f\"  Edges: {len(edges_df):,}\")\nprint(f\"  Node types: {nodes_df['node_type'].nunique()}\")\nprint(f\"  Edge types: {edges_df['relation'].nunique()}\")\n\n# Load ground truth\nprint(\"\\nLoading ground truth pathways...\")\ngt_nodes_df = pd.read_csv(PATHS['ground_truth_nodes'], dtype={'node_index': int})\ngt_edges_df = pd.read_csv(PATHS['ground_truth_edges'])\n\nprint(f\"  Ground truth node columns: {list(gt_nodes_df.columns)}\")\nprint(f\"  Ground truth edge columns: {list(gt_edges_df.columns)}\")\n\n# --- FILTER: keep only pathways with >= MIN_PATHWAY_NODES nodes ---\npathway_sizes = gt_nodes_df.groupby('pathway_id').size()\nvalid_pathways = pathway_sizes[pathway_sizes >= MIN_PATHWAY_NODES].index.tolist()\nremoved_pathways = pathway_sizes[pathway_sizes < MIN_PATHWAY_NODES]\n\ngt_nodes_df = gt_nodes_df[gt_nodes_df['pathway_id'].isin(valid_pathways)].reset_index(drop=True)\ngt_edges_df = gt_edges_df[gt_edges_df['pathway_id'].isin(valid_pathways)].reset_index(drop=True)\n\nprint(f\"\\n  Total pathways in file:    {len(pathway_sizes)}\")\nprint(f\"  Removed (< {MIN_PATHWAY_NODES} nodes):       {len(removed_pathways)}\")\nprint(f\"  Kept (>= {MIN_PATHWAY_NODES} nodes):          {len(valid_pathways)}\")\n\n# Show length distribution of kept pathways\nkept_sizes = gt_nodes_df.groupby('pathway_id').size()\nprint(f\"\\n  Pathway length distribution (kept):\")\nfor length in sorted(kept_sizes.unique()):\n    count = (kept_sizes == length).sum()\n    print(f\"    {length} nodes: {count} pathways\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Knowledge Graph\n\nConstruct a bidirectional NetworkX DiGraph from PrimeKG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes_df, edges_df, bidirectional=True):\n    \"\"\"\n    Build NetworkX DiGraph from PrimeKG CSVs.\n\n    nodes.csv columns: node_index, node_id, node_type, node_name, node_source\n    edges.csv columns: relation, display_relation, x_index, y_index\n    \"\"\"\n    G = nx.DiGraph()\n\n    for _, row in nodes_df.iterrows():\n        G.add_node(\n            int(row['node_index']),\n            node_id=str(row['node_id']),\n            node_name=str(row['node_name']),\n            node_type=str(row['node_type']),\n        )\n\n    for _, row in edges_df.iterrows():\n        src, dst = int(row['x_index']), int(row['y_index'])\n        rel = str(row['relation'])\n        disp = str(row['display_relation'])\n        G.add_edge(src, dst, relation=rel, display_relation=disp)\n        if bidirectional:\n            G.add_edge(dst, src, relation=rel, display_relation=disp)\n\n    return G\n\n\nprint(\"Building graph...\")\nG = build_graph(nodes_df, edges_df, bidirectional=True)\nprint(f\"\u2713 Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n\n# Precompute degree counts and hub threshold for evaluation\nprint(\"\\nComputing degree statistics...\")\ndegree_count = compute_degree_counts(edges_df)\nhub_threshold = compute_hub_threshold(degree_count, percentile=95)\nprint(f\"  Hub threshold (95th percentile): {hub_threshold:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run All Algorithms\n\nGeneric runner that takes any algorithm's `find_path(source, target)` method and produces a predictions DataFrame, then a unified orchestrator runs all five algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(algo_find_path_fn, graph, gt_nodes_df, algo_name, verbose=True):\n    \"\"\"\n    Generic runner: takes a find_path(source, target) callable and ground truth,\n    returns a DataFrame of predictions with timing.\n    \"\"\"\n    results = []\n    pathways = gt_nodes_df['pathway_id'].unique()\n    n_total = len(pathways)\n\n    for idx, pathway_id in enumerate(pathways):\n        pw = gt_nodes_df[gt_nodes_df['pathway_id'] == pathway_id].sort_values('step_order')\n        source_idx = int(pw.iloc[0]['node_index'])\n        target_idx = int(pw.iloc[-1]['node_index'])\n\n        # Progress indicator (every 25 pathways)\n        if verbose and idx % 25 == 0:\n            print(f\"  [{idx+1}/{n_total}] Processing pathways...\")\n\n        start_t = time.perf_counter()\n        try:\n            path, relations, cost = algo_find_path_fn(source_idx, target_idx)\n        except Exception as e:\n            path, relations, cost = [], [], float('inf')\n        elapsed_ms = (time.perf_counter() - start_t) * 1000\n\n        if path:\n            node_ids = [graph.nodes[idx_n].get('node_id', str(idx_n)) for idx_n in path]\n            node_names = [graph.nodes[idx_n].get('node_name', str(idx_n)) for idx_n in path]\n            results.append({\n                'pathway_id': pathway_id,\n                'algorithm': algo_name,\n                'predicted_node_indices': ','.join(map(str, path)),\n                'predicted_node_ids': ','.join(node_ids),\n                'predicted_node_names': ','.join(node_names),\n                'predicted_relations': ','.join(relations),\n                'predicted_length': len(path),\n                'ground_truth_length': len(pw),\n                'time_ms': elapsed_ms,\n            })\n        else:\n            results.append({\n                'pathway_id': pathway_id,\n                'algorithm': algo_name,\n                'predicted_node_indices': 'NONE',\n                'predicted_node_ids': 'NONE',\n                'predicted_node_names': 'NONE',\n                'predicted_relations': 'NONE',\n                'predicted_length': 0,\n                'ground_truth_length': len(pw),\n                'time_ms': elapsed_ms,\n            })\n\n    df = pd.DataFrame(results)\n    found = (df['predicted_length'] > 0).sum()\n    avg_ms = df['time_ms'].mean()\n    print(f\"  \u2713 {algo_name}: {found}/{n_total} paths found, avg {avg_ms:.1f}ms/pathway\")\n    return df\n\n\nprint(\"\u2713 Generic algorithm runner loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# RUN ALL 5 ALGORITHMS\n# ============================================================\nall_predictions = {}\nn_pathways = gt_nodes_df['pathway_id'].nunique()\nprint(f\"Running all algorithms on {n_pathways} pathways (>= {MIN_PATHWAY_NODES} nodes each)\")\nprint()\n\n# --- 1. Shortest Path (unweighted BFS) ---\nprint(\"=\" * 60)\nprint(\"ALGORITHM 1: Shortest Path (Unweighted)\")\nprint(\"=\" * 60)\n\ndef shortest_path_find(source, target):\n    try:\n        path = nx.shortest_path(G, source, target)\n        relations = []\n        for i in range(len(path) - 1):\n            ed = G.get_edge_data(path[i], path[i + 1])\n            relations.append(ed.get('relation', 'unknown') if ed else 'unknown')\n        return path, relations, len(path)\n    except nx.NetworkXNoPath:\n        return [], [], float('inf')\n\nall_predictions['Shortest Path'] = run_algorithm(shortest_path_find, G, gt_nodes_df, 'Shortest Path')\n\n\n# --- 2. Hub-Penalized ---\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ALGORITHM 2: Hub-Penalized (\u03b1=0.5)\")\nprint(\"=\" * 60)\n\nhub_algo = HubPenalizedShortestPath(G, alpha=0.5)\nall_predictions['Hub-Penalized'] = run_algorithm(hub_algo.find_path, G, gt_nodes_df, 'Hub-Penalized')\n\n\n# --- 3. PageRank-Inverse ---\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ALGORITHM 3: PageRank-Inverse\")\nprint(\"=\" * 60)\n\npr_algo = PageRankInverseShortestPath(G, damping=0.85)\nall_predictions['PageRank-Inverse'] = run_algorithm(pr_algo.find_path, G, gt_nodes_df, 'PageRank-Inverse')\n\n\n# --- 4. Learned A* ---\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ALGORITHM 4: Learned Embeddings + A*\")\nprint(\"=\" * 60)\n\nastar_algo = LearnedEmbeddingsAStar(G, embedding_dim=64)\nastar_algo.train_embeddings()\n\n# Prepare training data from ground truth\ntraining_pathways = []\nfor pid in gt_nodes_df['pathway_id'].unique():\n    pw = gt_nodes_df[gt_nodes_df['pathway_id'] == pid].sort_values('step_order')\n    training_pathways.append({'path_nodes': pw['node_index'].tolist()})\nastar_algo.train_edge_weights(training_pathways)\n\nall_predictions['Learned A*'] = run_algorithm(astar_algo.find_path, G, gt_nodes_df, 'Learned A*')\n\n\n# --- 5. Semantic Bridging ---\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ALGORITHM 5: Semantic Bridging (\u03b2=0.3)\")\nprint(\"=\" * 60)\n\nsem_algo = SemanticBridgingPath(G, beta=0.3)\nsem_algo.compute_embeddings()\nsem_algo.compute_edge_weights()\n\nall_predictions['Semantic Bridging'] = run_algorithm(sem_algo.find_path, G, gt_nodes_df, 'Semantic Bridging')\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"\u2713 All 5 algorithms complete on {n_pathways} pathways.\")\nprint(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate All Algorithms\n\nCompute all evaluation metrics for every algorithm \u00d7 pathway combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_predictions(all_preds, gt_nodes_df, gt_edges_df, degree_count, hub_threshold):\n    \"\"\"\n    Evaluate all algorithm predictions against ground truth.\n    Returns a single DataFrame with all metrics.\n    \"\"\"\n    # Auto-detect the edge type column name in ground truth edges\n    edge_type_col = None\n    for candidate in ['relation_type', 'relation', 'edge_type', ':TYPE']:\n        if candidate in gt_edges_df.columns:\n            edge_type_col = candidate\n            break\n    if edge_type_col is None:\n        print(f\"  \u26a0 Could not find edge type column. Available: {list(gt_edges_df.columns)}\")\n        print(f\"    Falling back to first column: {gt_edges_df.columns[0]}\")\n        edge_type_col = gt_edges_df.columns[0]\n    print(f\"  Using edge type column: '{edge_type_col}'\")\n\n    all_results = []\n    total = sum(len(df) for df in all_preds.values())\n    processed = 0\n\n    for algo_name, pred_df in all_preds.items():\n        for _, pred_row in pred_df.iterrows():\n            pathway_id = pred_row['pathway_id']\n            processed += 1\n\n            if processed % 200 == 0:\n                print(f\"  [{processed}/{total}] evaluating...\")\n\n            # Ground truth\n            gt_pw = gt_nodes_df[gt_nodes_df['pathway_id'] == pathway_id].sort_values('step_order')\n            gt_ed = gt_edges_df[gt_edges_df['pathway_id'] == pathway_id]\n\n            gt_node_ids = [str(x) for x in gt_pw['node_id'].tolist()]\n            gt_target_id = str(gt_pw.iloc[-1]['node_id'])\n            gt_edge_types = gt_ed[edge_type_col].tolist() if not gt_ed.empty else []\n\n            # Parse predictions\n            if pred_row['predicted_node_ids'] == 'NONE':\n                pred_node_ids, pred_indices, pred_relations = [], [], []\n            else:\n                pred_node_ids = pred_row['predicted_node_ids'].split(',')\n                pred_indices = [int(x) for x in pred_row['predicted_node_indices'].split(',')]\n                pred_relations = pred_row['predicted_relations'].split(',') if pred_row['predicted_relations'] != 'NONE' else []\n\n            # Compute metrics\n            p = metric_precision(pred_node_ids, gt_node_ids)\n            r = metric_recall(pred_node_ids, gt_node_ids)\n            f1 = metric_f1(pred_node_ids, gt_node_ids)\n            hits = calculate_hits_at_k(pred_node_ids, gt_target_id)\n            rel_acc = calculate_relation_accuracy(pred_relations, gt_edge_types)\n            edit_dist = calculate_edit_distance(pred_node_ids, gt_node_ids)\n            hub_ratio = calculate_hub_node_ratio(pred_indices, degree_count, hub_threshold)\n            len_mae = calculate_path_length_mae(pred_row['predicted_length'], pred_row['ground_truth_length'])\n            path_len_acc = metric_path_length_accuracy(pred_row['predicted_length'], pred_row['ground_truth_length'])\n            m = metric_mrr(pred_node_ids, gt_node_ids)\n\n            all_results.append({\n                'pathway_id': pathway_id,\n                'algorithm': algo_name,\n                'precision': p,\n                'recall': r,\n                'f1_score': f1,\n                'hits_at_1': hits['hits_at_1'],\n                'hits_at_3': hits['hits_at_3'],\n                'hits_at_5': hits['hits_at_5'],\n                'relation_type_accuracy': rel_acc,\n                'path_edit_distance': edit_dist,\n                'hub_node_ratio': hub_ratio,\n                'path_length_mae': len_mae,\n                'path_length_accuracy': path_len_acc,\n                'mrr': m,\n                'time_ms': pred_row['time_ms'],\n            })\n\n    return pd.DataFrame(all_results)\n\n\nprint(\"Evaluating all algorithms...\")\neval_df = evaluate_all_predictions(all_predictions, gt_nodes_df, gt_edges_df, degree_count, hub_threshold)\nprint(f\"\u2713 Evaluation complete: {len(eval_df)} rows ({eval_df['algorithm'].nunique()} algorithms \u00d7 {eval_df['pathway_id'].nunique()} pathways)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics per algorithm\nmetric_cols = ['precision', 'recall', 'f1_score', 'hits_at_1', 'hits_at_3', 'hits_at_5',\n               'relation_type_accuracy', 'path_edit_distance', 'hub_node_ratio',\n               'path_length_mae', 'path_length_accuracy', 'mrr', 'time_ms']\n\nsummary = eval_df.groupby('algorithm')[metric_cols].mean().round(4)\n\nprint(\"=\" * 90)\nprint(f\"ALGORITHM COMPARISON \u2014 Mean Metrics Across {eval_df['pathway_id'].nunique()} Pathways\")\nprint(\"=\" * 90)\nprint(summary.T.to_string())\nprint()\n\n# Best per metric\nprint(\"\\nBest algorithm per metric:\")\nprint(\"-\" * 60)\nfor col in metric_cols:\n    if col in ['path_edit_distance', 'hub_node_ratio', 'path_length_mae', 'time_ms']:\n        best = summary[col].idxmin()  # lower is better\n        val = summary[col].min()\n        direction = \"\u2193\"\n    else:\n        best = summary[col].idxmax()  # higher is better\n        val = summary[col].max()\n        direction = \"\u2191\"\n    print(f\"  {col:<28s} {direction}  {best} ({val:.4f})\")\n\n# Paths found\nprint(\"\\nPaths Found:\")\nfor alg in summary.index:\n    pred = all_predictions[alg]\n    found = (pred['predicted_length'] > 0).sum()\n    total = len(pred)\n    print(f\"  {alg:<20s}: {found}/{total} ({found/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# VISUALIZATION: Multi-panel comparison\n# ============================================================\n\nalgo_names = list(summary.index)\nn_algos = len(algo_names)\ncolors_list = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12'][:n_algos]\ncolor_map = dict(zip(algo_names, colors_list))\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle(f'Pathfinding Algorithm Benchmark \u2014 {eval_df[\"pathway_id\"].nunique()} Pathways (\u2265{MIN_PATHWAY_NODES} nodes)',\n             fontsize=15, fontweight='bold')\n\n# --- 1. Node Accuracy ---\nax = axes[0, 0]\nnode_metrics = ['precision', 'recall', 'f1_score']\nx = np.arange(len(node_metrics))\nwidth = 0.15\nfor i, alg in enumerate(algo_names):\n    vals = [summary.loc[alg, m] for m in node_metrics]\n    ax.bar(x + i * width, vals, width, label=alg, color=color_map[alg])\nax.set_ylabel('Score')\nax.set_title('Node Accuracy')\nax.set_xticks(x + width * (n_algos - 1) / 2)\nax.set_xticklabels(['Precision', 'Recall', 'F1'])\nax.legend(fontsize=7)\nax.set_ylim(0, 1.1)\nax.grid(axis='y', alpha=0.3)\n\n# --- 2. Mechanistic Quality ---\nax = axes[0, 1]\nmech_metrics = ['relation_type_accuracy', 'hub_node_ratio', 'path_edit_distance']\nx = np.arange(len(mech_metrics))\nfor i, alg in enumerate(algo_names):\n    vals = [summary.loc[alg, m] for m in mech_metrics]\n    ax.bar(x + i * width, vals, width, label=alg, color=color_map[alg])\nax.set_ylabel('Score')\nax.set_title('Mechanistic Quality')\nax.set_xticks(x + width * (n_algos - 1) / 2)\nax.set_xticklabels(['Relation Acc \u2191', 'Hub Ratio \u2193', 'Edit Dist \u2193'], fontsize=9)\nax.legend(fontsize=7)\nax.set_ylim(0, 1.1)\nax.grid(axis='y', alpha=0.3)\n\n# --- 3. Target Finding ---\nax = axes[1, 0]\nhits_metrics = ['hits_at_1', 'hits_at_3', 'hits_at_5']\nx = np.arange(len(hits_metrics))\nfor i, alg in enumerate(algo_names):\n    vals = [summary.loc[alg, m] for m in hits_metrics]\n    ax.bar(x + i * width, vals, width, label=alg, color=color_map[alg])\nax.set_ylabel('Score')\nax.set_title('Target Finding (Hits@k)')\nax.set_xticks(x + width * (n_algos - 1) / 2)\nax.set_xticklabels(['Hits@1', 'Hits@3', 'Hits@5'])\nax.legend(fontsize=7)\nax.set_ylim(0, 1.1)\nax.grid(axis='y', alpha=0.3)\n\n# --- 4. Radar Chart ---\naxes[1, 1].set_visible(False)\nax_radar = fig.add_subplot(2, 2, 4, projection='polar')\n\nradar_metrics = ['precision', 'recall', 'f1_score', 'hits_at_1', 'relation_type_accuracy', 'mrr']\nradar_labels = ['Precision', 'Recall', 'F1', 'Hits@1', 'Rel Acc', 'MRR']\n\nangles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()\nangles += angles[:1]\n\nfor alg in algo_names:\n    vals = [summary.loc[alg, m] for m in radar_metrics] + [summary.loc[alg, radar_metrics[0]]]\n    ax_radar.plot(angles, vals, 'o-', linewidth=2, label=alg, color=color_map[alg])\n    ax_radar.fill(angles, vals, alpha=0.1, color=color_map[alg])\n\nax_radar.set_xticks(angles[:-1])\nax_radar.set_xticklabels(radar_labels, fontsize=8)\nax_radar.set_ylim(0, 1)\nax_radar.set_title('Performance Profile', pad=20)\nax_radar.legend(loc='upper right', bbox_to_anchor=(1.4, 1.1), fontsize=7)\n\nplt.tight_layout()\nplt.savefig('algorithm_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"\u2713 Saved: algorithm_comparison.png\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# TIMING COMPARISON\n# ============================================================\n\navg_times = eval_df.groupby('algorithm')['time_ms'].mean().sort_values()\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = ax.barh(avg_times.index, avg_times.values, color=[color_map.get(a, '#999') for a in avg_times.index])\nax.set_xlabel('Average Time per Pathway (ms)')\nax.set_title('Algorithm Speed Comparison')\nfor bar, val in zip(bars, avg_times.values):\n    ax.text(bar.get_width() + max(avg_times.values)*0.01, bar.get_y() + bar.get_height()/2,\n            f'{val:.1f}ms', va='center', fontsize=9)\nax.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig('timing_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"\u2713 Saved: timing_comparison.png\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance by Pathway Length\n\nAnalyze whether algorithm performance varies with ground truth pathway length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth length to eval_df for grouping\neval_with_len = eval_df.copy()\n\n# Make sure ground truth length is available\ngt_len_map = gt_nodes_df.groupby('pathway_id').size().to_dict()\neval_with_len['gt_length'] = eval_with_len['pathway_id'].map(gt_len_map)\n\n# Group by length bucket\neval_with_len['length_bucket'] = eval_with_len['gt_length'].apply(\n    lambda x: '4 nodes' if x == 4 else ('5 nodes' if x == 5 else ('6 nodes' if x == 6 else '7+ nodes'))\n)\n\nprint(\"Pathway count by length:\")\nfor bucket in ['4 nodes', '5 nodes', '6 nodes', '7+ nodes']:\n    count = eval_with_len[eval_with_len['length_bucket'] == bucket]['pathway_id'].nunique()\n    if count > 0:\n        print(f\"  {bucket}: {count} pathways\")\n\n# F1 by length and algorithm\nprint(\"\\nMean F1 by pathway length:\")\npivot = eval_with_len.pivot_table(index='algorithm', columns='length_bucket', values='f1_score', aggfunc='mean').round(4)\nprint(pivot.to_string())\n\n# Plot F1 by length\nfig, ax = plt.subplots(figsize=(10, 6))\nbuckets = [b for b in ['4 nodes', '5 nodes', '6 nodes', '7+ nodes'] if b in pivot.columns]\nx = np.arange(len(buckets))\nwidth = 0.15\n\nfor i, alg in enumerate(algo_names):\n    if alg in pivot.index:\n        vals = [pivot.loc[alg, b] if b in pivot.columns else 0 for b in buckets]\n        ax.bar(x + i * width, vals, width, label=alg, color=color_map[alg])\n\nax.set_ylabel('Mean F1 Score')\nax.set_title('F1 Score by Ground Truth Pathway Length')\nax.set_xticks(x + width * (n_algos - 1) / 2)\nax.set_xticklabels(buckets)\nax.legend(fontsize=8)\nax.set_ylim(0, 1.0)\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig('f1_by_length.png', dpi=300, bbox_inches='tight')\nprint(\"\\n\u2713 Saved: f1_by_length.png\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Detailed Pathway Analysis (Sample)\n\nInspect a sample of individual pathway results across all algorithms. Shows first 10 pathways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"DETAILED PATHWAY ANALYSIS (first 10 pathways)\")\nprint(\"=\" * 80)\n\nalgo_list = list(all_predictions.keys())\nsample_pathways = gt_nodes_df['pathway_id'].unique()[:10]\n\nfor pathway_id in sample_pathways:\n    print(f\"\\n{'=' * 70}\")\n    print(f\"Pathway: {pathway_id}\")\n    print(f\"{'=' * 70}\")\n\n    # Ground truth\n    gt = gt_nodes_df[gt_nodes_df['pathway_id'] == pathway_id].sort_values('step_order')\n    gt_path = ' \u2192 '.join(gt['node_name'].tolist())\n    gt_types = ' \u2192 '.join(gt['node_type'].tolist())\n    print(f\"\\n  Ground Truth ({len(gt)} nodes):\")\n    print(f\"    {gt_path}\")\n    print(f\"    Types: {gt_types}\")\n\n    # Each algorithm\n    for alg_name in algo_list:\n        pred = all_predictions[alg_name]\n        row = pred[pred['pathway_id'] == pathway_id]\n        if row.empty:\n            continue\n        row = row.iloc[0]\n        if row['predicted_node_names'] != 'NONE':\n            pred_path = row['predicted_node_names'].replace(',', ' \u2192 ')\n            n_nodes = row['predicted_length']\n        else:\n            pred_path = '(no path found)'\n            n_nodes = 0\n        print(f\"\\n  {alg_name} ({n_nodes} nodes, {row['time_ms']:.1f}ms):\")\n        print(f\"    {pred_path}\")\n\n    # Metrics table\n    pw_eval = eval_df[eval_df['pathway_id'] == pathway_id]\n    if not pw_eval.empty:\n        show_metrics = ['f1_score', 'relation_type_accuracy', 'path_edit_distance', 'hub_node_ratio']\n        header = f\"  {'Metric':<28s}\" + \"\".join(f\"{a:>18s}\" for a in algo_list)\n        print(f\"\\n{header}\")\n        print(f\"  {'-' * (28 + 18 * len(algo_list))}\")\n        for metric in show_metrics:\n            row_str = f\"  {metric:<28s}\"\n            for alg in algo_list:\n                val = pw_eval[pw_eval['algorithm'] == alg][metric].values\n                row_str += f\"{val[0]:>18.3f}\" if len(val) > 0 else f\"{'N/A':>18s}\"\n            print(row_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\neval_df.to_csv('evaluation_results_all_algorithms.csv', index=False)\nprint(f\"\u2713 Saved: evaluation_results_all_algorithms.csv ({len(eval_df)} rows)\")\n\n# Save summary\nsummary.to_csv('algorithm_summary.csv')\nprint(\"\u2713 Saved: algorithm_summary.csv\")\n\n# Save per-algorithm predictions\nfor alg_name, pred_df in all_predictions.items():\n    fname = f\"predictions_{alg_name.lower().replace(' ', '_').replace('*', 'star')}.csv\"\n    pred_df.to_csv(fname, index=False)\n    print(f\"\u2713 Saved: {fname} ({len(pred_df)} rows)\")\n\nprint(f\"\\n\u2713 All outputs saved! ({eval_df['pathway_id'].nunique()} pathways \u00d7 {eval_df['algorithm'].nunique()} algorithms)\")\n"
   ]
  }
 ]
}