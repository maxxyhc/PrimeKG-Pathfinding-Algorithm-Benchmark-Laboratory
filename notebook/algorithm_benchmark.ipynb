{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Repurposing Pathfinding Algorithm Benchmark\n",
    "\n",
    "**Purpose:** Evaluate graph pathfinding algorithms for drug repurposing by comparing predicted mechanistic pathways against curated ground truth pathways.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import heapq\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import deque, Counter\n",
    "from evaluation_helpers import *\n",
    "from evaluation_metrics import *\n",
    "from Algorithms import *\n",
    "from evaluation_runner import run_evaluation\n",
    "import importlib\n",
    "import evaluation_visualization as ev\n",
    "importlib.reload(ev)\n",
    "from hyperparameter_tuning import (\n",
    "    run_full_grid_search,\n",
    "    plot_tuning_results,\n",
    "    get_best_predictions\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Detect repository root (folder that contains \"data\")\n",
    "REPO_ROOT = Path.cwd()\n",
    "\n",
    "# If running from /notebook, move up one level\n",
    "if not (REPO_ROOT / \"data\").exists() and (REPO_ROOT.parent / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "PATHS = {\n",
    "    \"nodes\": DATA_DIR / \"nodes.csv\",\n",
    "    \"edges\": DATA_DIR / \"edges.csv\",\n",
    "    \"ground_truth_nodes\": DATA_DIR / \"benchmark_pathways_nodes.csv\",\n",
    "    \"ground_truth_edges\": DATA_DIR / \"benchmark_pathways_edges.csv\",\n",
    "}\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "for k, p in PATHS.items():\n",
    "    print(f\"{k}: {p} | exists={p.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data\n",
    "\n",
    "Load the PrimeKG knowledge graph and ground truth pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PrimeKG data\n",
    "print(\"Loading PrimeKG data...\")\n",
    "nodes = pd.read_csv(PATHS['nodes'], encoding=\"latin1\")\n",
    "edges = pd.read_csv(PATHS['edges'], encoding=\"latin1\")\n",
    "\n",
    "print(f\"  Nodes: {len(nodes):,}\")\n",
    "print(f\"  Edges: {len(edges):,}\")\n",
    "print(f\"  Node types: {nodes['node_type'].nunique()}\")\n",
    "print(f\"  Edge types: {edges['relation'].nunique()}\")\n",
    "\n",
    "# Load ground truth\n",
    "print(\"\\nLoading ground truth pathways...\")\n",
    "ground_truth_nodes = pd.read_csv(PATHS['ground_truth_nodes'], dtype={'node_index': int})\n",
    "ground_truth_edges = pd.read_csv(PATHS['ground_truth_edges'])\n",
    "\n",
    "pathways = ground_truth_nodes['pathway_id'].unique()\n",
    "print(f\"  Pathways: {len(pathways)}\")\n",
    "for p in pathways:\n",
    "    n_nodes = len(ground_truth_nodes[ground_truth_nodes['pathway_id'] == p])\n",
    "    print(f\"    - {p}: {n_nodes} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "node_counts = ground_truth_nodes.groupby('pathway_id')['node_index'].count()\n",
    "long_pathways = node_counts[node_counts >= 4].index\n",
    "\n",
    "ground_truth_nodes = ground_truth_nodes[ground_truth_nodes['pathway_id'].isin(long_pathways)]\n",
    "ground_truth_edges = ground_truth_edges[ground_truth_edges['pathway_id'].isin(long_pathways)]\n",
    "\n",
    "print(f\"Filtered to {len(long_pathways)} pathways with 4+ nodes\")\n",
    "print(f\"  Nodes: {len(ground_truth_nodes):,}\")\n",
    "print(f\"  Edges: {len(ground_truth_edges):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Knowledge Graph\n",
    "\n",
    "Construct a NetworkX directed graph with node/edge attributes for pathfinding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(nodes_df, edges_df, bidirectional=True):\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from cleaned PrimeKG CSVs.\n",
    "    \n",
    "    nodes_df columns:\n",
    "        node_index, node_id, node_type, node_name, node_source\n",
    "        \n",
    "    edges_df columns:\n",
    "        relation, display_relation, x_index, y_index\n",
    "    \"\"\"\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # ---------- Add nodes ----------\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        G.add_node(\n",
    "            int(row['node_index']),\n",
    "            node_id=str(row['node_id']),\n",
    "            node_name=str(row['node_name']),\n",
    "            node_type=str(row['node_type']),\n",
    "            node_source=str(row['node_source'])\n",
    "        )\n",
    "    \n",
    "    # ---------- Add edges ----------\n",
    "    for _, row in edges_df.iterrows():\n",
    "        G.add_edge(\n",
    "            int(row['x_index']),\n",
    "            int(row['y_index']),\n",
    "            relation=str(row['relation']),\n",
    "            display_relation=str(row['display_relation'])\n",
    "        )\n",
    "        \n",
    "        if bidirectional:\n",
    "            G.add_edge(\n",
    "                int(row['y_index']),\n",
    "                int(row['x_index']),\n",
    "                relation=str(row['relation']),\n",
    "                display_relation=str(row['display_relation'])\n",
    "            )\n",
    "    \n",
    "    return G\n",
    "\n",
    "print(\"Building graph...\")\n",
    "G = build_graph(nodes, edges, bidirectional=True)\n",
    "print(f\"Graph built: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm 1 - Shortest Path Baseline\n",
    "\n",
    "The simplest baseline: find the shortest path (by hop count) between drug and disease.\n",
    "\n",
    "**Expected behavior:**\n",
    "- ✅ Will always find the target disease (if connected)\n",
    "- ❌ May take shortcuts through direct drug→disease edges\n",
    "- ❌ Ignores edge types and biological mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shortest_path(graph, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Run shortest path algorithm on all pathways.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions for each pathway\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        # Get source (drug) and target (disease) indices\n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        \n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Find shortest path\n",
    "            predicted_path = nx.shortest_path(graph, source_idx, target_idx)\n",
    "            predicted_node_ids = [graph.nodes[idx]['node_id'] for idx in predicted_path]\n",
    "            predicted_node_names = [graph.nodes[idx]['node_name'] for idx in predicted_path]\n",
    "            \n",
    "            # Get edge relations along path\n",
    "            predicted_relations = []\n",
    "            for i in range(len(predicted_path) - 1):\n",
    "                edge_data = graph.get_edge_data(predicted_path[i], predicted_path[i+1])\n",
    "                predicted_relations.append(edge_data['relation'])\n",
    "            \n",
    "            print(f\"  ✓ Found path: {len(predicted_path)} nodes\")\n",
    "            print(f\"  Path: {' → '.join(predicted_node_names[:5])}{'...' if len(predicted_path) > 5 else ''}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, predicted_path)),\n",
    "                'predicted_node_ids': ','.join(predicted_node_ids),\n",
    "                'predicted_node_names': ','.join(predicted_node_names),\n",
    "                'predicted_relations': ','.join(predicted_relations),\n",
    "                'predicted_length': len(predicted_path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "            \n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"  ✗ No path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run shortest path\n",
    "print(\"=\"*60)\n",
    "print(\"Running Shortest Path Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sp_predictions = run_shortest_path(G, ground_truth_nodes)\n",
    "# sp_predictions.to_csv('baseline_shortest_path_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: baseline_shortest_path_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Meta-Path Constrained BFS\n",
    "\n",
    "An improved baseline that enforces biologically valid edge type sequences.\n",
    "\n",
    "**Valid meta-path patterns:**\n",
    "1. `drug → protein → disease` (direct mechanism)\n",
    "2. `drug → protein → protein → disease` (protein interactions)\n",
    "3. `drug → protein → anatomy → protein → disease` (tissue-specific)\n",
    "\n",
    "**Invalid shortcuts blocked:**\n",
    "- ❌ `drug → disease` (clinical indication, not mechanism)\n",
    "- ❌ `drug → drug → disease` (drug similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valid meta-path patterns (edge type sequences)\n",
    "VALID_METAPATHS = [\n",
    "    # Pattern 1: Direct protein mechanism (drug → protein → disease)\n",
    "    ['drug_protein', 'disease_protein'],\n",
    "    \n",
    "    # Pattern 2: Protein-protein interaction\n",
    "    ['drug_protein', 'protein_protein', 'disease_protein'],\n",
    "    \n",
    "    # Pattern 3: Multiple protein interactions\n",
    "    ['drug_protein', 'protein_protein', 'protein_protein', 'disease_protein'],\n",
    "    \n",
    "    # Pattern 4: Pathway-mediated\n",
    "    ['drug_protein', 'pathway_protein', 'disease_protein'],\n",
    "    ['drug_protein', 'pathway_protein', 'pathway_protein', 'disease_protein'],\n",
    "    ['drug_protein', 'pathway_protein', 'pathway_pathway', 'pathway_protein', 'disease_protein'],\n",
    "    \n",
    "    # Pattern 5: Anatomy-mediated (tissue-specific)\n",
    "    ['drug_protein', 'anatomy_protein_present', 'anatomy_protein_present', 'disease_protein'],\n",
    "    \n",
    "    # Pattern 6: Complex pathways\n",
    "    ['drug_protein', 'protein_protein', 'pathway_protein', 'disease_protein'],\n",
    "    ['drug_protein', 'pathway_protein', 'pathway_protein', 'pathway_protein', 'disease_protein'],\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(VALID_METAPATHS)} valid meta-path patterns:\")\n",
    "for i, pattern in enumerate(VALID_METAPATHS, 1):\n",
    "    print(f\"  {i}. {' → '.join(pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_metapath(relations, valid_metapaths):\n",
    "    \"\"\"Check if a relation sequence matches any valid meta-path pattern.\"\"\"\n",
    "    return relations in valid_metapaths\n",
    "\n",
    "\n",
    "def could_match_metapath(relations, valid_metapaths):\n",
    "    \"\"\"Check if the current relation sequence could potentially lead to a valid path.\"\"\"\n",
    "    for pattern in valid_metapaths:\n",
    "        if len(relations) <= len(pattern):\n",
    "            if relations == pattern[:len(relations)]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def metapath_constrained_bfs(source_idx, target_idx, graph, valid_metapaths, max_length=10):\n",
    "    \"\"\"\n",
    "    Find shortest path that follows valid meta-path patterns.\n",
    "    \n",
    "    Uses BFS but only explores edges that could lead to a valid meta-path.\n",
    "    \n",
    "    Returns:\n",
    "        (path_nodes, path_relations) or ([], []) if no valid path found\n",
    "    \"\"\"\n",
    "    # Queue: (current_node, path_so_far, relations_so_far)\n",
    "    queue = deque([(source_idx, [source_idx], [])])\n",
    "    visited = {source_idx: []}  # Track visited states with relation sequences\n",
    "    \n",
    "    while queue:\n",
    "        current_node, path, relations = queue.popleft()\n",
    "        \n",
    "        # Check if we reached target with valid meta-path\n",
    "        if current_node == target_idx:\n",
    "            if is_valid_metapath(relations, valid_metapaths):\n",
    "                return path, relations\n",
    "        \n",
    "        # Stop if path too long\n",
    "        if len(path) >= max_length:\n",
    "            continue\n",
    "        \n",
    "        # Explore neighbors\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            edge_data = graph.get_edge_data(current_node, neighbor)\n",
    "            new_relation = edge_data['relation']\n",
    "            new_relations = relations + [new_relation]\n",
    "            \n",
    "            # Only continue if this could lead to a valid meta-path\n",
    "            if could_match_metapath(new_relations, valid_metapaths):\n",
    "                state_key = (neighbor, tuple(new_relations))\n",
    "                \n",
    "                # Avoid revisiting same state\n",
    "                if neighbor not in visited or visited[neighbor] != new_relations:\n",
    "                    visited[neighbor] = new_relations\n",
    "                    queue.append((neighbor, path + [neighbor], new_relations))\n",
    "    \n",
    "    return [], []  # No valid path found\n",
    "\n",
    "\n",
    "def run_metapath_algorithm(graph, ground_truth_df, valid_metapaths):\n",
    "    \"\"\"\n",
    "    Run meta-path constrained BFS on all pathways.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        \n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        gt_path = ' → '.join(pathway_df['node_name'].tolist())\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        # Find meta-path constrained path\n",
    "        predicted_path, predicted_relations = metapath_constrained_bfs(\n",
    "            source_idx, target_idx, graph, valid_metapaths\n",
    "        )\n",
    "        \n",
    "        if predicted_path:\n",
    "            predicted_node_ids = [graph.nodes[idx]['node_id'] for idx in predicted_path]\n",
    "            predicted_node_names = [graph.nodes[idx]['node_name'] for idx in predicted_path]\n",
    "            \n",
    "            print(f\"  ✓ Found valid path: {len(predicted_path)} nodes\")\n",
    "            print(f\"  Meta-path: {' → '.join(predicted_relations)}\")\n",
    "            print(f\"  Path: {' → '.join(predicted_node_names)}\")\n",
    "            print(f\"  Ground truth: {gt_path}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, predicted_path)),\n",
    "                'predicted_node_ids': ','.join(predicted_node_ids),\n",
    "                'predicted_node_names': ','.join(predicted_node_names),\n",
    "                'predicted_relations': ','.join(predicted_relations),\n",
    "                'predicted_length': len(predicted_path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✗ No valid meta-path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run meta-path algorithm\n",
    "print(\"=\"*60)\n",
    "print(\"Running Meta-Path Constrained Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mp_predictions = run_metapath_algorithm(G, ground_truth_nodes, VALID_METAPATHS)\n",
    "# mp_predictions.to_csv('baseline_metapath_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: baseline_metapath_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Hub-Penalized Weighted Shortest Path\n",
    "\n",
    "**Core Idea:** High-degree \"hub\" nodes (like inflammation markers) connect to everything but don't represent specific mechanisms. Penalize them.\n",
    "\n",
    "**Weight Formula:** `weight[u,v] = 1 + α * log(degree[v])`\n",
    "\n",
    "- α = 0.5 is a good default (can be tuned)\n",
    "- Higher degree → higher weight → less preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALGORITHM 2: Hub-Penalized Weighted Shortest Path\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def run_hub_penalized(graph, ground_truth_df, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Run Hub-Penalized algorithm on all pathways.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initialize algorithm\n",
    "    print(\"Initializing Hub-Penalized algorithm...\")\n",
    "    algo = HubPenalizedShortestPath(graph, alpha=alpha)\n",
    "    print(f\"  Edge weights computed (α={alpha})\")\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        path, relations, weight = algo.find_path(source_idx, target_idx)\n",
    "        \n",
    "        if path:\n",
    "            node_ids = [graph.nodes[idx].get('node_id', str(idx)) for idx in path]\n",
    "            node_names = [graph.nodes[idx].get('node_name', str(idx)) for idx in path]\n",
    "            \n",
    "            print(f\"  ✓ Found path: {len(path)} nodes\")\n",
    "            print(f\"  Path: {' → '.join(node_names)}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, path)),\n",
    "                'predicted_node_ids': ','.join(node_ids),\n",
    "                'predicted_node_names': ','.join(node_names),\n",
    "                'predicted_relations': ','.join(relations),\n",
    "                'predicted_length': len(path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✗ No path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run Algorithm 2\n",
    "print(\"=\"*60)\n",
    "print(\"Running Hub-Penalized Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hub_predictions = run_hub_penalized(G, ground_truth_nodes, alpha=0.5)\n",
    "# hub_predictions.to_csv('hub_penalized_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: hub_penalized_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3: PageRank-Inverse Weighted Shortest Path\n",
    "\n",
    "**Core Idea:** PageRank captures global graph centrality. Nodes with HIGH PageRank are generic hubs. We want paths through LOW PageRank (more specific) nodes.\n",
    "\n",
    "**Weight Formula:** `weight[u,v] = 1 / (1 + pagerank[v])`\n",
    "\n",
    "- Low PageRank → low weight → preferred\n",
    "- PageRank is computed once upfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALGORITHM 3: PageRank-Inverse Weighted Shortest Path\n",
    "# ============================================================\n",
    "\n",
    "def run_pagerank_inverse(graph, ground_truth_df, damping=0.85):\n",
    "    \"\"\"\n",
    "    Run PageRank-Inverse algorithm on all pathways.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initialize algorithm\n",
    "    print(\"Initializing PageRank-Inverse algorithm...\")\n",
    "    algo = PageRankInverseShortestPath(graph, damping=damping)\n",
    "    print(f\"  Edge weights computed\")\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        path, relations, weight = algo.find_path(source_idx, target_idx)\n",
    "        \n",
    "        if path:\n",
    "            node_ids = [graph.nodes[idx].get('node_id', str(idx)) for idx in path]\n",
    "            node_names = [graph.nodes[idx].get('node_name', str(idx)) for idx in path]\n",
    "            \n",
    "            print(f\"  ✓ Found path: {len(path)} nodes\")\n",
    "            print(f\"  Path: {' → '.join(node_names)}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, path)),\n",
    "                'predicted_node_ids': ','.join(node_ids),\n",
    "                'predicted_node_names': ','.join(node_names),\n",
    "                'predicted_relations': ','.join(relations),\n",
    "                'predicted_length': len(path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✗ No path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run Algorithm 3\n",
    "print(\"=\"*60)\n",
    "print(\"Running PageRank-Inverse Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pr_predictions = run_pagerank_inverse(G, ground_truth_nodes, damping=0.85)\n",
    "# pr_predictions.to_csv('pagerank_inverse_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: pagerank_inverse_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 4: Learned Embeddings + A* with Supervised Edge Weights\n",
    "\n",
    "**Core Idea:** Learn from known drug repurposing pathways what makes a \"good\" edge.\n",
    "\n",
    "**Two Phases:**\n",
    "1. **Embed:** Train Node2Vec (or use spectral embeddings) to capture graph structure\n",
    "2. **Learn:** Train MLP to predict edge goodness from:\n",
    "   - Embedding similarity\n",
    "   - Degree features\n",
    "   - Edge type\n",
    "\n",
    "**Search:** A* with learned weights + embedding-based heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALGORITHM 4: Learned Embeddings + A* with Supervised Edge Weights\n",
    "# ============================================================\n",
    "\n",
    "def run_learned_astar(graph, ground_truth_df, embedding_dim=64):\n",
    "    \"\"\"\n",
    "    Run Learned Embeddings + A* algorithm on all pathways.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initialize algorithm\n",
    "    print(\"Initializing Learned Embeddings + A* algorithm...\")\n",
    "    algo = LearnedEmbeddingsAStar(graph, embedding_dim=embedding_dim)\n",
    "    algo.train_embeddings()\n",
    "    \n",
    "    # Prepare training data from ground truth\n",
    "    training_pathways = []\n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        training_pathways.append({'path_nodes': pathway_df['node_index'].tolist()})\n",
    "    \n",
    "    algo.train_edge_weights(training_pathways)\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        path, relations, weight = algo.find_path(source_idx, target_idx)\n",
    "        \n",
    "        if path:\n",
    "            node_ids = [graph.nodes[idx].get('node_id', str(idx)) for idx in path]\n",
    "            node_names = [graph.nodes[idx].get('node_name', str(idx)) for idx in path]\n",
    "            \n",
    "            print(f\"  ✓ Found path: {len(path)} nodes\")\n",
    "            print(f\"  Path: {' → '.join(node_names)}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, path)),\n",
    "                'predicted_node_ids': ','.join(node_ids),\n",
    "                'predicted_node_names': ','.join(node_names),\n",
    "                'predicted_relations': ','.join(relations),\n",
    "                'predicted_length': len(path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✗ No path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run Algorithm 4\n",
    "print(\"=\"*60)\n",
    "print(\"Running Learned Embeddings + A* Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "learned_predictions = run_learned_astar(G, ground_truth_nodes, embedding_dim=64)\n",
    "# learned_predictions.to_csv('learned_astar_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: learned_astar_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 5: Semantic Bridging with Intermediate Node Scoring\n",
    "\n",
    "**Core Idea:** Use NLP to find paths where consecutive nodes are semantically related (they \"make sense\" together).\n",
    "\n",
    "**Weight Formula:** `weight[u,v] = 1 - β * cosine_sim(text_emb[u], text_emb[v])`\n",
    "\n",
    "- β = 0.3 balances semantic preference with path length\n",
    "- Uses TF-IDF embeddings (or SciBERT if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALGORITHM 5: Semantic Bridging with Intermediate Node Scoring\n",
    "# ============================================================\n",
    "\n",
    "def run_semantic_bridging(graph, ground_truth_df, beta=0.3):\n",
    "    \"\"\"\n",
    "    Run Semantic Bridging algorithm on all pathways.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initialize algorithm\n",
    "    print(\"Initializing Semantic Bridging algorithm...\")\n",
    "    algo = SemanticBridgingPath(graph, beta=beta)\n",
    "    algo.compute_embeddings()\n",
    "    algo.compute_edge_weights()\n",
    "    \n",
    "    for pathway_id in ground_truth_df['pathway_id'].unique():\n",
    "        pathway_df = ground_truth_df[ground_truth_df['pathway_id'] == pathway_id].sort_values('step_order')\n",
    "        \n",
    "        source_idx = int(pathway_df.iloc[0]['node_index'])\n",
    "        target_idx = int(pathway_df.iloc[-1]['node_index'])\n",
    "        source_name = pathway_df.iloc[0]['node_name']\n",
    "        target_name = pathway_df.iloc[-1]['node_name']\n",
    "        \n",
    "        print(f\"\\n{pathway_id}: {source_name} → {target_name}\")\n",
    "        \n",
    "        path, relations, weight = algo.find_path(source_idx, target_idx)\n",
    "        \n",
    "        if path:\n",
    "            node_ids = [graph.nodes[idx].get('node_id', str(idx)) for idx in path]\n",
    "            node_names = [graph.nodes[idx].get('node_name', str(idx)) for idx in path]\n",
    "            \n",
    "            print(f\"  ✓ Found path: {len(path)} nodes\")\n",
    "            print(f\"  Path: {' → '.join(node_names)}\")\n",
    "            \n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': ','.join(map(str, path)),\n",
    "                'predicted_node_ids': ','.join(node_ids),\n",
    "                'predicted_node_names': ','.join(node_names),\n",
    "                'predicted_relations': ','.join(relations),\n",
    "                'predicted_length': len(path),\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✗ No path found\")\n",
    "            results.append({\n",
    "                'pathway_id': pathway_id,\n",
    "                'predicted_node_indices': 'NONE',\n",
    "                'predicted_node_ids': 'NONE',\n",
    "                'predicted_node_names': 'NONE',\n",
    "                'predicted_relations': 'NONE',\n",
    "                'predicted_length': 0,\n",
    "                'ground_truth_length': len(pathway_df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run Algorithm 5\n",
    "print(\"=\"*60)\n",
    "print(\"Running Semantic Bridging Algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "semantic_predictions = run_semantic_bridging(G, ground_truth_nodes, beta=0.3)\n",
    "# semantic_predictions.to_csv('semantic_bridging_predictions.csv', index=False)\n",
    "# print(\"\\n✓ Saved: semantic_bridging_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate Algorithms\n",
    "\n",
    "Calculate all 9 metrics for both algorithms and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {\n",
    "    'Dijkstra': sp_predictions,\n",
    "    'Meta-Path': mp_predictions,\n",
    "    'Hub-Penalized': hub_predictions,\n",
    "    'PageRank-Inverse': pr_predictions,\n",
    "    'Semantic-Bridging': semantic_predictions,\n",
    "}\n",
    "\n",
    "print(edges.columns.tolist())\n",
    "\n",
    "results, summary = run_evaluation(\n",
    "    predictions_dict=predictions_dict,\n",
    "    ground_truth_nodes=ground_truth_nodes,\n",
    "    ground_truth_edges=ground_truth_edges,\n",
    "    edges_df=edges\n",
    ")\n",
    "\n",
    "# 用新的 visualization\n",
    "ev.display_summary_table(summary)\n",
    "\n",
    "\n",
    "# results.to_csv('evaluation_results.csv', index=False)\n",
    "# summary.to_csv('evaluation_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for all tunable algorithms\n",
    "best_configs, tuning_results = run_full_grid_search(\n",
    "    G,                          # Your NetworkX graph\n",
    "    ground_truth_nodes,         # Ground truth nodes DataFrame\n",
    "    ground_truth_edges,         # Ground truth edges DataFrame\n",
    "    edges,                      # PrimeKG edges DataFrame\n",
    "    hub_alphas=[0.1, 0.3, 0.5, 0.7, 1.0, 1.5],   # Alpha values to test for Hub-Penalized\n",
    "    pr_dampings=[0.7, 0.85, 0.9],                 # Damping values to test for PageRank\n",
    "    semantic_betas=[0.1, 0.3, 0.5, 0.7]          # Beta values to test for Semantic-Bridging\n",
    ")\n",
    "\n",
    "\n",
    "# Generate predictions using the optimal parameters found\n",
    "best_predictions = get_best_predictions(G, ground_truth_nodes, best_configs)\n",
    "\n",
    "# Combine with non-tunable algorithms (Dijkstra, Meta-Path)\n",
    "predictions_dict = {\n",
    "    'Dijkstra': sp_predictions,\n",
    "    'Meta-Path': mp_predictions,\n",
    "    **best_predictions  # Adds tuned Hub-Penalized, PageRank-Inverse, Semantic-Bridging\n",
    "}\n",
    "\n",
    "# Run final evaluation with all algorithms\n",
    "results, summary = run_evaluation(\n",
    "    predictions_dict=predictions_dict,\n",
    "    ground_truth_nodes=ground_truth_nodes,\n",
    "    ground_truth_edges=ground_truth_edges,\n",
    "    edges_df=edges\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary\n",
    "\n",
    "Compare algorithm performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics per algorithm\n",
    "metrics = ['precision', 'recall', 'f1_score', 'hits_at_1', 'relation_type_accuracy', \n",
    "           'hub_node_ratio', 'path_edit_distance', 'path_length_mae']\n",
    "\n",
    "summary = all_eval.groupby('algorithm')[metrics].mean().round(3)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALGORITHM COMPARISON: Average Metrics Across All Pathways\")\n",
    "print(\"=\"*70\n",
    "print(summary.T.to_string())\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Count perfect matches\n",
    "print(\"\\nPerfect Matches (Edit Distance = 0):\")\n",
    "for alg in ['Shortest Path', 'Meta-Path']:\n",
    "    perfect = (all_eval[all_eval['algorithm'] == alg]['path_edit_distance'] == 0).sum()\n",
    "    total = len(all_eval[all_eval['algorithm'] == alg])\n",
    "    print(f\"  {alg}: {perfect}/{total} pathways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
